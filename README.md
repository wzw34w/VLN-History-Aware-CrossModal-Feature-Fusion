# VLN-History-Aware-CrossModal-Feature-Fusion
Vision-and-Language Navigation Based on History-Aware CrossModal  Feature Fusion in Indoor Environment 
    
    Shuhuan Wen, Simeng Gong, Ziyuan Zhang, F. Richard Yu, Zhiwen Wang

 ## Abstract
Vision-and-language navigation (VLN) is a challenging task that requires
an agent to navigate through an indoor environment based on natural language instructions. Traditional VLN uses cross-modal feature fusion that
visual and textual information are combined to guide the agent’s navigation. However, incomplete utilization of perceptual information, scarcity of
domain-specific training data, and diverse image and language inputs result
in suboptimal performance. In this paper, we propose a cross-modal feature
fusion VLN with history-aware information, which leverages the agent’s past
experiences to make more informed navigation decisions. Regretful model
and self-monitoring model are added, advantage actor critic(A2C) reinforcement learning algorithm is used to improve navigation success rate, reduce
action redundancy, and shorten navigation paths. Then the data augmentation method based on Speaker is introduced to improve the generalization
of the model. We evaluate the proposed algorithm on the Room-to-Room
(R2R) and Room-for-Room (R4R), and the experimental results show thatit outperforms existing models by comparing the state-of-art methods.

## Notes
This repository is used to display the scientific results of the paper "Vision-and-Language Navigation Based on History-Aware CrossModal  Feature Fusion in Indoor Environment".
