# VLN-History-Aware-CrossModal-Feature-Fusion
Vision-and-Language Navigation Based on History-Aware CrossModal  Feature Fusion in Indoor Environment 
    Shuhuan Wen, Simeng Gong, F. Richard Yu, Zhiwen Wang

 ## Abstract
 Vision-and-language navigation (VLN) is a challenging task that requires
an agent to navigate through an indoor environment based on natural language instructions. Traditional VLN uses cross-modal feature fusion that visual and textual information are combined to guide the agent’s navigation. However, incomplete utilization of perceptual information, scarcity of domain-specifc training data, and diverse image and language inputs result in suboptimal performance. In this paper, we propose a cross-modal feature fusion VLN with history-aware information, which leverages the agent’s past experiences to make more informed navigation decisions. Regretful model and self-monitoring model are added, advantage actor critic(A2C) reinforcement learning algorithm is used to improve navigation success rate, reduce action redundancy, and shorten navigation paths. Then the data augmentation method based on Speaker is introduced to improve the generalization of the model. We evaluate the proposed algorithm on the Room-to-Room (R2R) and Room-for-Room (R4R), and the experimental results show that it outperforms existing models by comparing the state-of-art methods.

## Notes
This repository is used to display the scientific results of the paper "Vision-and-Language Navigation Based on History-Aware CrossModal  Feature Fusion in Indoor Environment".
